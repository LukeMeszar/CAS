This is Info file syntax, produced by Makeinfo-1.43 from the input
file /home/lukem/Dropbox/CAS/eli-4.8.1/Eli/pkg/parser/syntax.tnf.


File: syntax,  Node: Conflicts,  Next: Actions,  Prev: Mapping,  Up: Top

How to Resolve Parsing Conflicts
********************************

   Eli attempts to construct a particular kind of parser from the
context-free grammar specifying the desired phrase structure.  If this
attempt fails, Eli reports that failure by describing a set of
"conflicts".  In order to understand what these conflicts mean, and to
understand how they might be resolved, it is necessary to have a
rudimentary idea of how the constructed parser determines the phrase
structure of the input text.

   A context-free grammar is said to be "ambiguous" if it permits more
than one phrase structure to describe a single input text.  Most
conflicts are the result of such ambiguities, and there are three ways
of resolving them:

  1. Change the grammar so that only one phrase structure is possible.

  2. Provide additional information that causes the parser to select
     one of the set of phrase structures.

  3. Change the form of the input text to avoid the ambiguity.

Note that all of these methods result in the parser recognizing a
different language than the one described by the original grammar.

* Menu:

* Parsing::	How the generated parser determines phrase structure
* Changes::	Conflict resolution by changing the grammar
* Modifiers::	Conflict resolution by ignoring possible structures


File: syntax,  Node: Parsing,  Next: Changes,  Up: Conflicts

How the generated parser determines phrase structure
====================================================

   The generated parser is a finite-state machine with a stack of
states.  This machine examines the input text from left to right, one
basic symbol at a time.  The current state of the machine is the one
at the top of the stack.  It defines the set of productions the parser
might be recognizing, and its progress in recognizing each.  For
example, consider the following trivial grammar:

     Sentence: Expression.
     Expression: Primary.
     Expression: Expression '+' Primary.
     Primary: Integer.
     Primary: Id.

   Initially, the parser might be recognizing the first production,
but in order to do so it must recognize either the second or the third. 
In order to recognize the second production, it must recognize either
the fourth or fifth.  Finally, because we are considering the initial
situation, no progress has been made in recognizing any of these
productions.  All of the information expressed by this paragraph is
represented by the initial state, which is the only element of the
stack.

   On the basis of the state at the top of the stack, and the basic
symbol being examined, the machine decides on one of two moves:

`Shift'
     Accept the basic symbol as the corresponding terminal, push a new
     state onto the stack, and examine the next basic symbol.

`Reduce'
     Note that a specific phrase has been recognized, remove a number
     of states equal to the number of symbols in the sequence of the
     corresponding production from the stack, push a new state onto
     the stack, and examine the current basic symbol again.

The parser halts after the reduce move noting that the production
containing the axiom has been recognized.

   If the first basic symbol of the text were an identifier, a parser
for the sample grammar would make a shift move.  The new state would
be one in which the parser had completely recognized the fifth
production.  Regardless of the next basic symbol, the parser would
then make a reduce move because the fifth production has been
recognized.  One state would be removed from the stack, and a new
state pushed in which the the parser had completely recognized the
second production.  Again the parser would make a reduce move,
removing one state from the stack and pushing a state in which the
parser had either completely recognized the first production or
recognized the first symbol of the third production.

   The parser's next move is determined by the current input symbol. 
If the text is empty then the parser makes the reduce move noting that
the first production has been recognized and halts.  If the current
symbol of the text is '+' then the parser makes a shift move.

   A conflict occurs when the information available (the current state
and the basic symbol being examined) does not allow the parser to make
a unique decision.  If either a shift or a reduce is possible, the
conflict is a "shift-reduce conflict"; if more than one phrase could
have been recognized, the conflict is a "reduce-reduce conflict".

* Menu:

* Shift-reduce::	Example of a shift-reduce conflict
* Reduce-reduce::	Example of a reduce-reduce conflict


File: syntax,  Node: Shift-reduce,  Next: Reduce-reduce,  Up: Parsing

Example of a shift-reduce conflict
----------------------------------

   The classic example of a shift-reduce conflict is the so-called
"dangling else problem":

     Statement: 'if' Expression 'then' Statement.
     Statement: 'if' Expression 'then' Statement 'else' Statement.

   A parser built from a grammar containing these productions will
have at least one state in which it could be recognizing either, and
has just completed recognition of the `Statement' following `then'. 
Suppose that the current basic symbol is `else'; what move should the
parser make next?

   Clearly it could shift, accepting the `else' and going to a state
in which it is recognizing the second production and has just
completed recognition of the `else'.  It could also reduce, however,
recognizing an instance of the first production, popping four elements
from the stack and returning to the current state.  Thus there is a
shift-reduce conflict.

   The conflict here is due to an ambiguity in the grammar.  Consider
the following input text (E1 and E2 are arbitrary expressions, S1 and
S2 are statements that do not contain `if'):

     if E1 then if E2 then S1 else S2

There are two possible phrase structures for this text, depending on
whether the `else' is assumed to belong with the first or second `if':

     if E1 then {if E2 then S1} else S2
     if E1 then {if E2 then S1 else S2}

In each case the bracketed sub-sequence is a `Statement' according to
one of the given rules, and the entire line is a `Statement' according
to the other.  Both are perfectly legal phrase structures according to
the grammar.


File: syntax,  Node: Reduce-reduce,  Prev: Shift-reduce,  Up: Parsing

Example of a shift-reduce conflict
----------------------------------

   The following description of integer denotations in various bases
leads to a reduce-reduce conflict:

     Denotation: Seq / Seq Base.
     Seq: Digit / Seq Next.
     Next: Digit / Hexit.
     Digit: '0' / '1' / '2' / '3' / '4' / '5' / '6' / '7' / '8' / '9'.
     Hexit: 'a' / 'b' / 'c' / 'd' / 'e' / 'f'.
     Base: 'b' / 'o' / 'e' / 'x'.

When `Base' is omitted, the integer is assumed to be decimal if it
contains no `Hexit' and hexadecimal otherwise.  An explicit `Base'
indicates the base of the digits to be 2, 8, 10 or 16 respectively.

   One of the states of the parser constructed from this grammar
indicates that either the `Hexit' `b' or the `Base' `b' has been
recognized.  If the input is not empty then the parser has recognized
a `Hexit', but either is possible if the input is empty.  Thus the
parser cannot determine the production by which to reduce, and the
conflict arises.

   This conflict indicates an ambiguity in the grammar, exemplified by
the input text "1b".  Two phrase structures are possible, one yielding
the value "1 base 2" and the other yielding the value "1b base 16".


File: syntax,  Node: Changes,  Next: Modifiers,  Prev: Parsing,  Up: Conflicts

Conflict resolution by changing the grammar
===========================================

   An ambiguity can sometimes be resolved by changing the grammar. 
The altered grammar must define exactly the same set of input texts as
the grammar that gave rise to the conflict, but it cannot describe
more than one phrase structure for any particular text.  That phrase
structure must reflect the meaning of the text as defined by the
language design.

   Most languages solve the dangling else problem by associating an
`else' with the closest `if'.  Here is an unambiguous grammar
describing that phrase structure:

     Statement: matched / unmatched.
     matched:
        'if' Expression 'then' matched 'else' matched /
        Others.
     unmatched:
        'if' Expression 'then' matched 'else' unmatched /
        'if' Expression 'then' Statement.

(`Others' stands for all sequences by which `Statement' could be
replaced that contain no `if'.)

   If the identifiers `Statement', `matched' and `unmatched' are
placed in an equivalence class, then this grammar yields exactly the
same phrase structure as the ambiguous grammar given in the previous
section.  It is therefore acceptable as far as the remainder of the
translation problem is concerned.


File: syntax,  Node: Modifiers,  Prev: Changes,  Up: Conflicts

Conflict resolution by ignoring possible structures
===================================================

   When Eli is constructing a parser from a grammar, it computes a set
of symbols called the "exact right context" for each production in
each state.  The exact right context of a production in a state
contains all of the symbols that could follow the phrase associated
with that production in that state.  It is possible for the parser to
reduce by a production if the current state indicates that all of the
symbols in the production's sequence have been accepted, and the next
basic symbol of the input is a member of the exact right context of
that production in that state.

   By adding a "modification" to the description of a production in a
type-`con' file, the user can specify that a particular symbol be
deleted from one or more exact right contexts.  The user is, in
effect, telling Eli that these symbols cannot follow the phrase
associated with that production in that state.  In other words, the
parser is to ignore phrase structures in which the specified symbol
follows the phrase.

   A modification is a sequence consisting of either a dollar (`$') or
at-rate-of (`@') followed by a terminal.  It can be placed anywhere
within a production, and more than one modification can appear in a
single production.  If a modification is introduced but no conflict is
resolved thereby, an error is reported.

* Menu:

* Dollar::	The effect of a $-modification
* At::		The effect of a @-modification


File: syntax,  Node: Dollar,  Next: At,  Up: Modifiers

The effect of a $-modification
------------------------------

   Suppose that a modification `$S' is introduced into a production
`P'.  The effect of this modification is to delete the symbol `S' from
the exact right context of production `P'.  This kind of modification
can be used to solve the dangling else problem:

     Statement: 'if' Expression 'then' Statement $'else'.
     Statement: 'if' Expression 'then' Statement 'else' Statement.

The modification introduced into the first production removes `else'
from the exact right context of that production, and therefore makes a
reduce move impossible for the parser when it is in the state
indicating that it is recognizing one of these productions and has
just recognized the first `Statement'.  Since the reduce move is
impossible, there is no shift-reduce conflict.


File: syntax,  Node: At,  Prev: Dollar,  Up: Modifiers

The effect of a @-modification
------------------------------

   Suppose that a modification `@S' is introduced into a production
`P'.  The effect of this modification is to delete the symbol `S' from
the exact right context of any production involved in a reduce-reduce
conflict with production `P'.  This kind of modification can be used
to solve the integer denotation problem:

     Denotation: Seq / Seq Base.
     Seq: Digit / Seq Next.
     Next: Digit / Hexit.
     Digit: '0' / '1' / '2' / '3' / '4' / '5' / '6' / '7' / '8' / '9'.
     Hexit: 'a' / 'b' / 'c' / 'd' / 'e' / 'f'.
     Base: 'b' @EOF / 'o' / 'e' @EOF / 'x'.

The two modifications introduced into the productions remove `EOF'
(the empty input text) from the exact right contexts of the `Hexit'
productions that conflict with these two `Base' productions, and
therefore make it impossible to reduce the `Hexit' productions when
the parser is in the state indicating it has completed recognizing
either a `Hexit' or `Base' and the input is empty.  A `b' or `e' at
the end of an input text will thus always be interpreted as a marker:
"1b" means "1 base 2", not "1b base 16".  ("1b base 16" would have to
be written as "1bx".)


File: syntax,  Node: Actions,  Next: Error Recovery,  Prev: Conflicts,  Up: Top

Carrying Out Actions During Parsing
***********************************

   In some cases the translation problem being solved requires that
arbitrary actions be carried out as the parser is recognizing the
phrase structure of the input, rather than waiting for the complete
phrase structure to be available.  Most of those cases can be
classified either as interactive applications or as complex
structuring problems in which contextual information is needed to
determine the phrase structure.

   An arbitrary action is specified by a fragment of C code.  None of
the data accessed by this code is provided by Eli; it is the
responsibility of the writer of the arbitrary actions to manage any
data they manipulate.  The simplest approach is to implement all
actions as invocations of operators exported by a library or
user-defined abstract data type.  If these invocations have arguments,
they are either constant values characteristic of the particular
invocation or references to an entity exported by some (possibly
different) abstract data type.

   An action is a sequence consisting of an ampersand (`&') followed
by a literal.  The content of the literal is the C code fragment to be
executed.  Actions can be placed anywhere in a production, and will be
executed when all of the symbols to the left of the action's position
have been recognized.  Thus an action placed at the end of the
production would be executed when all of the symbols in the sequence
have been recognized.

   Here is a fragment of a grammar describing a desk calculator;
actions are used to compute subexpression values as the expression is
parsed:

     expression:
        term /
        expression '+' term &'ExprPlus();' /
        expression '-' term &'ExprMinus();' .
     
     term:
        primary /
        term '*' primary &'ExprTimes();' /
        term '/' primary &'ExprDiv();' .

The C code fragments invoke operations of a module that maintains a
stack of integer values.

   If an action is placed anywhere other than the end of the
production, it may lead to conflicts.  Suppose that an action is
placed between the first and second symbols of the sequence in a
production `P'.  Suppose further that there is another production,
`Q', whose sequence begins with the same two symbols but does not
contain the same action.  If one of the states of the parser could be
recognizing either `P' or `Q', and has recognized the first symbol, it
would not be able to decide whether or not to execute the action.


File: syntax,  Node: Error Recovery,  Next: Foreign,  Prev: Actions,  Up: Top

Improving Error Recovery in the Generated Parser
************************************************

   In some cases, the same pattern in the input text may represent
different tokens in the grammar.  Knowing which token the pattern
represents may be based on other available information.  When the
parser determines that it cannot accept the next look-ahead token, the
boolean function `Reparatur' is called:

     int Reparatur (POSITION *coord, int *syncode, int *intrinsic);

   This allows the user to change the look-ahead token based on other
available information.  If the function returns `0', then the token
has not been altered and the generated parser continues with its
normal error recovery.  If the function returns `1', it is assumed
that the passed in attributes of the token have been changed (in
particular `syncode'), and the generated parser rechecks the
look-ahead token to see if it can accept it.

   By default, the Eli system provides file `dfltrepar.c' containing a
definition of the function `Reparatur' that always returns `0'.  To
override the default, the user must provide a new definition of the
function `Reparatur' in some C file.

   In case of erroneous input the generated parser invokes its error
recovery.  The error recovery works completely automatically and
usually behaves satisfactorily, in that it produces a tree that is
close to the one that might be expected if there were no syntactic
errors.  This enables the compiler to go on and detect additional
semantic errors.

   It is also possible to generate a program that will terminate after
parsing if syntactic errors were detected.  To generate a program with
this property, simply add the following parameter to the request for
derivation (*note define: (pp)define.):

     +define='STOPAFTERBADPARSE'

   There are a few possibilities to control the error recovery in
order to improve its behavior.  To understand the control facilities
it is necessary to know how the error recovery works in principle.

   If an error in the input is detected two methods for error repair
are used.  The first method tries to "correct" the error by deleting,
inserting, or replacing one input symbol.  The repair is considered
successful, if the next 4 parsing steps don't lead to another error. 
The use of this method is optional.  If the first method is not used
or if it failed the second method performs a complete correction
without backtracking.  It skips input symbols until a so-called
"restart point" is reached.  The restart point is a symbol where
normal parsing can be resumed.  Before normal parsing resumes error
correction takes place.  Input symbols are inserted in order to
construct a syntactically correct input and the associated semantic
actions are executed.  The intention is to pass consistent information
to the following compiler phases, which therefore do not have to
bother with syntax errors.

   The second method for error recovery can be controlled by providing
additional information.  The intention is to decrease the probability
of error avalanches caused by wrong error repair decisions.  As a
running example, we use an ALGOL-like language defined by the following
grammar:

     block : 'begin' declarations statements 'end' .
     declarations : declarations declaration ';' / .
     declaration : 'real' 'identifier' /
                     / 'procedure' 'identifier' ';' statement .
     statements : statement / statements ';' statement .
     statement : 'identifier' / block .

   Three types of error recovery information can be specified by the
user in files of type `.perr':

* Menu:

* Separators::           List Separators
* Brackets::             Semantic Brackets
* Skip Symbols::         Unsafe Restart Points


File: syntax,  Node: Separators,  Next: Brackets,  Up: Error Recovery

List Separators
===============

   The error recovery has a major drawback when applied to errors in
lists, defined, e.g., as

     statements : statement / statements ';' statement .

   A missing delimiter ';' cannot be inserted in order to parse the
rest of the list.  This could lead to an infinite loop in the parser. 
Therefore errors like

     begin identifier begin identifier ; ...

   cannot be repaired by inserting the semicolon ';' but by deleting
the two symbols 'begin' and 'identifier'.

   The following specification in a `.perr' file defines the mentioned
terminals as list separators.

     $SEPA ';' . ',' .

   A list separator will always be inserted if a restart point can be
found immediately behind it.  In this case the rest of the list can be
parsed without the danger of getting into an infinite loop.


File: syntax,  Node: Brackets,  Next: Skip Symbols,  Prev: Separators,  Up: Error Recovery

Semantic Brackets
=================

   Programming languages have bracketed structures like 'begin' and
'end' which delimit not only the syntactic structure of "block" but
also the scope of identifiers.  Deleting or inserting such
semantically significant parentheses is highly probably to cause
avalanches of syntactic and semantic errors.  Therefore, the error
recovery should not change the structures of a program as far as it
concerns scopes of identifiers or similar semantic concepts.

   Consider the following erroneous input:

     begin
       procedure identifier ;
       begin
         real identifier ;
         identifier ;
         real identifier ;
         identifier ;

   Inserting the terminal 'end' before the second "real declaration"
corrects the program syntactically but may lead to a semantic error in
the last line, as the scope structure is changed.

   The specification

     $BRACKET 'begin' . 'end' .

   in a file of type `.perr' declares the mentioned terminals to be
delimiters of semantically significant regions (semantic delimiters). 
These terminals are not inserted unless the restart point is end of
input or the restart point itself is specified as such a delimiter.


File: syntax,  Node: Skip Symbols,  Prev: Brackets,  Up: Error Recovery

Unsafe Restart Points
=====================

   Usually there are a few terminals not suited as restart points. 
The reason is that is programming languages terminals like
'identifier' or 'number' occur in many different syntactic positions. 
Consider the error

     begin real identifier identifier ; real identifier ...

   There is no safe way to tell whether the second identifier belongs
to a statement or to a declaration.  If it is used as a restart point,
the error is corrected to

     begin real identifier ; identifier ; real identifier ...

   This corresponds to a transition from the declaration part into the
statement part of the block, a frequent cause for error avalanches. 
In general, terminals like 'identifier' or 'number' are not feasible
as restart points.

   The specification

     $SKIP 'identifier' . 'integer_number' . 'real_number' .

   in a type `.perr' file defines the mentioned terminals as unsafe
restart points.  Unsafe restart points are skipped in case of an error
in order to search for restart points more feasible.

   With the above specification the second identifier in the mentioned
example will be skipped.  Parsing resumes at the following semicolon
without carrying out a transition to the statement part.


File: syntax,  Node: Foreign,  Next: Grammars,  Prev: Error Recovery,  Up: Top

Using Foreign parsers
*********************

   When Eli is used to generate a parser, Maptool is able to relate the
concrete syntax to the abstract syntax and create all of the code
necessary to build a tree representing the input text.  If a parser is
generated by other tools, or written by hand, tree-building code must
be created manually.  In this section, we assume that a parser for the
source language exists, and that Eli is being used to generate code
from a LIDO specification of the abstract syntax and desired tree
computations.

   The interface specification of any parser designed to support tree
computation defines a set of function invocations that will occur as
parsing of the input text proceeds.  If the parser has been generated,
these function invocations are included in the grammar as semantic
actions (*note Carrying Out Actions During Parsing: Actions.).

   The code generated from a LIDO specification includes a set of tree
construction functions, one for each rule context (*note Tree
Construction Functions: (lidoref)Tree Construction Functions.).  These
functions must be invoked at appropriate times with appropriate
arguments during the course of the parse.  In order to use an existing
parser, therefore, we must implement a module obeying the interface
specification of that parser and correctly invoking the tree
construction functions generated from the LIDO specification.

   It would be possible to develop the module in isolation and then
integrate it with the foreign parser, but a better approach is to use
the foreign parser as part of the Eli specification of the complete
program.  Development can then proceed incrementally using Eli tools
like execution monitoring to track down errors and verify correct
behavior.

* Menu:

* Nodes::        Building tree nodes
* Coordinates::  Adding coordinate information
* Lists::        Building LISTOF constructs
* Integration::  Running a foreign parser under Eli


File: syntax,  Node: Nodes,  Next: Coordinates,  Up: Foreign

Building tree nodes
===================

   A typical parser interface specifies a data structure to define text
fragments, in addition to the semantic actions:

     typedef struct {  /* Basic symbol */
       int line;       /*   Source line containing the symbol */
       int col;        /*   Column containing the first character */
       int type;       /*   Classification code of the symbol */
       char *text;     /*   Symbol text */
     } Token;
                       /* Symbol classification codes */
     #define ETXT 1    /*   End of the shource file */
     #define LPAR 2    /*   Left parenthesis */
     #define RPAR 3    /*   Right parenthesis */
     #define PLUS 4    /*   Plus */
     #define STAR 5    /*   Asterisk */
     #define INTG 6    /*   Integer */
     ...

   Each tree construction function generated by Eli from a LIDO
specification must be invoked with pointers to its children, and
therefore the tree must be built bottom-up.  The usual strategy is to
store pointers to constructed nodes on a stack until their parent node
is built.  Eli provides a stack module for this purpose:

     NODEPTR *_nst;         /* Stack array: _nst[...] are the elements */
     int _nsp;              /* Stack index: _nst[_nsp] is the top element */
     void _incrnodestack(); /* Push an empty element onto the stack */

Elements of the stack are `_nst[_nsp]', `_nst[_nsp-1]', etc.  The
statement `_nsp-=k;' pops `k' elements off of the stack, and the
statement `_incrnodestack();' pushes an empty element onto the stack. 
To make the stack visible, include the file `treestack.h'.

   The behavior of the functions called by the parser is determined
primarily by the needs of the abstract syntax.  We'll consider two
LIDO specifications, one for computing the value of an integer
expression involving addition and multiplication and the other for
carrying out overload resolution in more general expressions.

* Menu:

* Evaluate::    Tree designed for expression evaluation
* Overload::    Tree designed for overload resolution
* Chain nodes:: Tree nodes for chain rules


File: syntax,  Node: Evaluate,  Next: Overload,  Up: Nodes

Tree designed for expression evaluation
---------------------------------------

   Consider the following LIDO specification, which evaluates an
integer expression involving addition and multiplication.  It assumes
each `Integer' terminal is represented by the value of the
corresponding integer:

     ATTR val: int;
     
     RULE Top: Root ::= Expr COMPUTE
       printf("The value is %d\n", Expr.val);
     END;
     
     RULE Add: Expr ::= Expr '+' Expr COMPUTE
       Expr[1].val=ADD(Expr[2].val,Expr[3].val);
     END;
     
     RULE Mul: Expr ::= Expr '*' Expr COMPUTE
       Expr[1].val=MUL(Expr[2].val,Expr[3].val);
     END;
     
     RULE Num: Expr ::= Integer COMPUTE
       Expr.val=Integer;
     END;

Eli generates four node construction functions from this specification:

     NODEPTR MkTop(POSITION *_coord, NODEPTR _d1);
     NODEPTR MkAdd(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);
     NODEPTR MkMul(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);
     NODEPTR MkNum(POSITION *_coord, int _TERM1);

To make the node construction functions visible, include the file
`treecon.h'.

   The `_coord' parameter will be discussed in detail in the next
section; here we will always supply `NoPosition' as the value of this
argument (*note Source Text Coordinates and Error Reporting:
(lib)error.).

   Our module must call `MkNum' whenever the parser recognizes an
integer, and we must provide the internal value of that integer as the
second argument of that call.  The result of the call must be pushed
onto the top of the stack.

   Suppose that by looking at the code of the parser, or the grammar
from which the parser was generated, we determine that when the parser
recognizes an integer in the input text it calls the function
`int_literal_constant' with the `Token' describing that integer as its
argument.  We might then implement `int_literal_constant' as follows:

     void int_literal_constant(Token *t)
     { _incrnodestack();
       _nst[_nsp]=MkNum(NoPosition,atoi(t->text));
     }

Note that this code does *not* check for an error in the conversion of
the string.  That might or might not be reasonable, depending upon how
careful the parser was in accepting a string as a representation of an
integer value.

   Further examination of the parser might show that it calls the
function `mult_operand' with no arguments when it has recognized an
expression involving two operands and an asterisk operator.  In this
case, the nodes for the two operand expressions are already on the
stack.  They must be removed and replaced by a `Mul' node:

     void mult_operand(void)
     { _nst[_nsp-1]=MkMul(NoPosition,_nst[_nsp-1],_nst[_nsp]);
       _nsp--;
     }

Implementation of the action when the parser recognizes an expression
involving two operands and a plus operator is identical except that
`MkAdd' is invoked instead of `MkMul'.

   If the parser invokes `level_0_expr' with no arguments when it has
completed recognition of the input text, the implementation of that
function might be:

     void level_0_expr(void)
     { _nst[_nsp]=MkTop(NoPosition,_nst[_nsp]);
     }

   Suppose that the parser invokes `level_3_expr' with no arguments
when it has recognized an expression in parentheses.  There is no
corresponding rule in the abstract syntax, because parentheses serve
only to override operator precedence and do not affect the computation. 
In that case, the routine does nothing:

     void level_3_expr(void)
     { }

   An expression language usually has precedence levels containing
several operators.  For example, dyadic `+' and `-' operators usually
have the same precedence, as do dyadic `*' and `/'.  A parser may
invoke a single function when it recognizes *any* dyadic expression
whose operator is at a specific precedence level.  In that case, some
indication of the operator must be passed to that function.  For
example, the parser might call `mult_operand' with a pointer to the
operator token.  The implementation of `mult_operand' must then use
the token type to select the correct node construction function:

     void mult_operand(Token *o)
     { if (o->type == STAR)
         _nst[_nsp-1]=MkMul(NoPosition,_nst[_nsp-1],_nst[_nsp]);
       else
         _nst[_nsp-1]=MkDiv(NoPosition,_nst[_nsp-1],_nst[_nsp]);
       _nsp--;
     }

This assumes that the parser will not invoke `mult_operand' unless the
operator is either `*' or `/', and therefore no error checking is
required.  (If the number of operators at the precedence level were
larger, then a switch statement might be preferable to the
conditional.)

   The code for `mult_operand' also assumes that the division is
implemented by a LIDO rule named `Div':

     RULE Div: Expr ::= Expr '/' Expr COMPUTE
       Expr[1].val=DIV(Expr[2].val,Expr[3].val);
     END;


File: syntax,  Node: Overload,  Next: Chain nodes,  Prev: Evaluate,  Up: Nodes

Tree designed for overload resolution
-------------------------------------

   Consider the following LIDO specification, which provides a
structure to analyze the result type of an expression involving
addition, subtraction, multiplication, and division of integers and
floating point numbers (*note Operator Overloading:
(typetutorial)Operator.).  It assumes that each `Integer' and `Float'
terminal is represented by the string definining the corresponding
number:

     RULE Top: Root ::= Expr END;
     
     RULE Dya: Expr ::= Expr BinOp Expr END;
     
     RULE Pls: BinOp ::= '+' END;
     RULE Min: BinOp ::= '-' END;
     RULE Str: BinOp ::= '*' END;
     RULE Sls: BinOp ::= '/' END;
     
     RULE Ntg: Expr ::= Integer END;
     RULE Flt: Expr ::= Float   END;

Eli generates eight node construction functions from this
specification.  The first six are:

     NODEPTR MkTop(POSITION *_coord, NODEPTR _d1);
     NODEPTR MkDya(POSITION *_coord, NODEPTR _d1, NODEPTR _d2, NODEPTR _d3);
     NODEPTR MkPls(POSITION *_coord);
     NODEPTR MkMin(POSITION *_coord);
     NODEPTR MkAst(POSITION *_coord);
     NODEPTR MkSls(POSITION *_coord);

To make the node construction functions visible, include the file
`treenode.h'.

   In this example, we would like to represent the integer and
floating-point constants in the tree by the strings that represent
them in the source text.  There are two possibilities:

  1. If the foreign parser stores permanent copies of token strings,
     then pointers to those strings can be stored in the tree nodes.

  2. If the foreign parser points to token strings in the input
     buffer, then our module must store them permanently for reference
     by the tree nodes.

   In case 1, a specification must be added to the LIDO description of
the tree:

     TERM Integer, Float: CharPtr;

`CharPtr' is the LIDO name for the C type `char *'.  The definition of
`CharPtr' is made available by including file `strings.h'.

   The `TERM' specification causes the two functions `MxNtg' and
`MkFlt' to be defined as follows:

     NODEPTR MkNtg(POSITION *_coord, CharPtr t);
     NODEPTR MkFlt(POSITION *_coord, CharPtr t);

   Suppose that, as discussed in the last subsection, the parser calls
`int_literal_constant' when it recognizes an integer in the source
text.  That routine could be implemented as:

     void int_literal_constant(Token *t)
     { _incrnodestack();
       _nst[_nsp]=MkNtg(NoPosition,t->text);
     }

   In case 2, we can make use of Eli's `MakeName' module (*note
Generating Optional Identifiers: (problems)MakeName.).  It provides a
function to store a string uniquely and return an integer-valued hash
table index to that unique representation:

     int MakeName(char *c);

Because the default type of a LIDO terminal is `int', we can omit the
`TERM' specification and the two functions `MxNtg' and `MkFlt' will be
defined as follows:

     NODEPTR MkNtg(POSITION *_coord, int t);
     NODEPTR MkFlt(POSITION *_coord, int t);

The implementation of `int_literal_constant' would be:

     void int_literal_constant(Token *t)
     { _incrnodestack();
       _nst[_nsp]=MkNum(NoPosition,MakeName(t->text));
     }

   The `MakeName' module must be instantiated in order to gain access
to the `MakeName' function.  This is done by adding the following line
to a `.specs' file:

     $/Tech/MakeName.gnrc:inst

No `+instance' parameter should be supplied, because scanning and
parsing are provided by the foreign code.  Once the module has been
instantiated, the definition of the `MakeName' function is made
available to the tree construction module by including file
`MakeName.h'.

   Let's assume that the parser invokes a single function when it
recognizes any dyadic expression whose operator is at a specific
precedence level, passing the operator token to that function.  For
example, `+' and `-' might both be operators at precedence level 1:

     void level_1_operator(Token *o)
     { NODEPTR op;
       if (o->type == PLUS) op=MkPls(NoPosition);
       else                 op=MkMin(NoPosition);
       _nst[_nsp-1]=MkDya(NoPosition,_nst[_nsp-1],op,_nst[_nsp]);
       _nsp--;
     }

This assumes that the parser will not invoke `level_1_operator' unless
the operator is either `+' or `-', and therefore no error checking is
required.  (If the number of operators at the precedence level were
larger, then a switch statement might be preferable to the
conditional.)

   If, on the other hand, the parser invokes a function `add_operand'
with no arguments when it has recognized an expression involving two
operands and an addition operator then `add_operand' can be
implemented as:

     void add_operand(void)
     { _nst[_nsp-1]=
         MkDya(NoPosition,_nst[_nsp-1],MkPls(NoPosition),_nst[_nsp]);
       _nsp--;
     }

Note that the operator node implied by the `add_operand' call must be
explicitly created in this case; it is only implicit in the parse.


File: syntax,  Node: Chain nodes,  Prev: Overload,  Up: Nodes

Tree nodes for chain rules
--------------------------

   Recall that a chain rule has the form `X ::= Y', where `X' differs
from `Y' (*note Chain rule definitions: Chain Rules.).  Such a rule
will always result in a tree node with a single child, and if the rule
name is `Ch' then the constructor function will be:

     NODEPTR MkCh(POSITION *_coord, NODEPTR _d1);

   With the exception of the root node of the tree, it is never
necessary to explicitly invoke the constructor of a chain rule node. 
This is actually a very important property of the tree construction
module.  For example, consider the following fragment of a LIDO
specification:

     RULE SimpleVar: Var ::= VrblIdUse                      END;
     RULE SubscrVar: Var ::= Var '[' Exp ']'                END;
     RULE VarExp:    Exp ::= Var                            END;
     RULE ArrayExp:  Exp ::= TypeIdUse '[' Exp ']' 'of' Exp END;
     RULE Typ: TypeIdUse ::= Symbol                         END;
     RULE Var: VrblIdUse ::= Symbol                         END;
     RULE Idn:    Symbol ::= Identifier                     END;

`Identifier' is a terminal symbol, represented by a unique permanent
string (*note Tree designed for overload resolution: Overload.).

   The problem here is that, given the input sequence `a[', a parser
would have to look beyond the matching `]' in order to decide whether
`a' was a `VrblIdUse' or a `TypeIdUse'.  But because the rules `Typ'
and `Var' are chain rules, their constructor functions don't need to
be called.  That means the parser can construct an `Idn' node for `a'
and leave it on the stack.  If that node is later used as the left
child of a `SubscrVar' node, the tree construction module will insert
the necessary `Var' and `SimpleVar' nodes.  If, on the other hand, the
`Idn' node is used as the left child of an `ArrayExp' node then the
tree construction module will insert the necessary `Typ' node.  There
is no need for the parser to look ahead.


File: syntax,  Node: Coordinates,  Next: Lists,  Prev: Nodes,  Up: Foreign

Adding coordinate information
=============================

   LIDO computations may access the coordinates of the first character
of the source text region represented by a node.  Usually, these
computations are used to attach error reports to appropriate text
locations.  Many of the modules that implement common computations use
this facility for error reporting (for an example, *note Verifying
typed identifier usage: (type)ChkTyped.).

   Execution monitoring is provided by Noosa, a separate process that
can display the abstract syntax tree and graphically relate it to the
source text (*note Trees and Attribute Values: (mon)Trees.).  Noosa
requires that both the source text coordinates of the first character
of a tree context and those of the first character *beyond* that
context be supplied to its construction function.

   Specific source text coordinates are represented by a `POSITION'
(*note Source Text Coordinates and Error Reporting: (lib)error.). 
This data type and the operations upon it are made visible by
including the file `err.h'.  An appropriate `POSITION' value must be
created from parser data and a pointer to that data passed to the tree
construction function.

* Menu:

* Start::  Supplying coordinates for computation
* Span::   Supplying coordinates for Noosa


File: syntax,  Node: Start,  Next: Span,  Up: Coordinates

Supplying coordinates for computation
-------------------------------------

   LIDO provides three names that can be used in computations to
obtain source text coordinates of a tree context (*note Predefined
Entities: (lidoref)Predefined Entities.):

`LINE'
     the source line number of the tree context.

`COL'
     the source column number of the tree context.

`COORDREF'
     the address of the source coordinates of the tree context, to be
     used for example in calls of the message routine of the error
     module or in calls of tree construction functions.

If any of these three names appear in the LIDO computation, the source
text coordinates of the first character of each tree context must be
supplied to its node construction function.  That information must be
extracted from the parser.

   In order to support the use of coordinates in computation, the tree
construction function must have access to the location of the first
character of its tree context.  We have assumed that each token
provided by the parser specifies the line and column of the first
character of the corresponding input string (*note Building tree
nodes: Nodes.).  This information can be used to build a `POSITION'
value:

     POSITION curpos;
     
     void int_literal_constant(Token *t)
     { LineOf(curpos) = t->line; ColOf(curpos) = t->col;
       _incrnodestack();
       _nst[_nsp]=MkNum(&curpos,atoi(t->text));
     }

Notice that the address of `curpos', rather then `curpos' itself, is
passed to the node construction function `MkNum'.

   Unfortunately, this information isn't sufficient.  We must not only
pass the coordinates to `MkNum', we must also save them on the stack
in case this node is the left child of another node.  At that point,
the coordinates of the first character of this token would be the
coordinates of the first character of the larger tree context.

   The Eli stack module actually provides two parallel stacks, `_nst'
for nodes and `_pst' for positions.  Thus the complete code for
integer literal constants would be:

     void int_literal_constant(Token *t)
     { LineOf(curpos) = t->line; ColOf(curpos) = t->col;
       _incrnodestack();
       _pst[_nsp]=curpos;
       _nst[_nsp]=MkNum(&curpos,atoi(t->text));
     }

The position value, not a pointer to that value, is saved on the stack. 
That frees `curpos' to be used in constructing other values.

   When a node whose children are all on the stack is constructed, the
coordinates are obtained from the leftmost child:

     void mult_operand(void)
     { _nst[_nsp-1]=MkMul(&_pst[_nsp-1],_nst[_nsp-1],_nst[_nsp]);
       _nsp--;
     }

   Generally speaking, the stack location for the left operand becomes
the stack location for the result.  Because the coordinates of the
result are the coordinates of the left operand, there is no need for
an assignment to `_pst'.


File: syntax,  Node: Span,  Prev: Start,  Up: Coordinates

Supplying coordinates for Noosa
-------------------------------

   Noosa requires the coordinates of the first character of a tree
context and also the coordinates of the first character beyond the end
of that context.  The additional coordinates should be supplied,
however, *only* if execution monitoring has actually been specified
for the particular run.  This is because the `POSITION' value will
only have the necessary space if monitoring has been specified.

   The simplest strategy is to define a routine to compute the
appropriate `POSITION' value for a given token:

     POSITION PositionOf(Token_t *token)
     { POSITION curpos;
     
       LineOf(curpos) = token->line; ColOf(curpos) = token->col;
     #ifdef RIGHTCOORD
       RLineOf(curpos) = LineOf(curpos);
       RColOf(curpos) = ColOf(curpos) + strlen(token->text);
     #ifdef MONITOR
       CumColOf(curpos) = ColOf(curpos); RCumColOf(curpos) = RColOf(curpos);
     #endif
     #endif
       return curpos;
     }
   `RIGHTCOORD' and `MONITOR' are defined by Eli for each C
compilation if the user specifies the `+monitor' parameter to the
derivation (*note monitor: (pp)monitor.).

   A node for an integer literal constant would then be built by:

     void int_literal_constant(Token *t)
     { curpos = PositionOf(t);
       _incrnodestack();
       _pst[_nsp]=curpos;
       _nst[_nsp]=MkNum(&curpos,atoi(t->text));
     }

   The construction of a node whose children are on the stack becomes
more complex, because the coordinates of the constructed node involve
the coordinates of the first character of the leftmost child node and
the coordinates of the first character beyond the end of rightmost
child node.  The tree stack module provides a function, `SpanOf', to
compute the correct coordintes:

     POSITION SpanOf(POSITION left, POSITION right);

   Using `SpanOf', the `mult_operand' routine would be written as:

     void mult_operand(void)
     { curpos=SpanOf(_pst[_nsp-1],_pst[_nsp]);
        _pst[_nsp-1]=curpos;
        _nst[_nsp-1]=MkMul(&curpos,_nst[_nsp-1],_nst[_nsp]);
       _nsp--;
     }


File: syntax,  Node: Lists,  Next: Integration,  Prev: Coordinates,  Up: Foreign

Building LISTOF constructs
==========================

   There are *three* tree construction functions associated with a
LISTOF construct with the rule name `Ex' (*note Tree Construction
Functions: (lidoref)Tree Construction Functions.):

     NODEPTR MkEx(POSITION *_coord, NODEPTR _d1);
     NODEPTR Mk0Ex(POSITION *_coord);
     NODEPTR Mk2Ex(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);

Arguments `_d1' and `_d2' may be:

   * the result of `Mk0Ex', which represents an empty portion of the
     list (any call to `Mk0Ex' can be replaced by the constant
     `NULLNODEPTR')

   * the result of `Mk2Ex', which represents a portion (possibly
     empty) of the list

   * any node that can be made a list element subtree by implicit
     insertion of chain contexts, which represents a single element of
     the list

The node representing the complete `Ex' construct is the one resulting
from a call of `MkEx'.

   LISTOF constructs always involve either looping or recursion in a
parser.  For example, consider a language in which a block consists of
an arbitrary non-empty sequence of declarations and statements.  The
LIDO specification for the abstract syntax might contain the rule:

     RULE Blk: Block LISTOF Declaration | Statement END;

   Suppose that the parser calls `declaration_action' after each
`Declaration' has been recognized and `statement_action' after each
`Statement' has been recognized.  Moreover, it calls `block_begin'
prior to beginning analysis of the list and `block_end' when the end
of the block has been reached:

     void block_begin(void)
     { _incrnodestack();
       _nst[_nsp]=Mk0Blk(&curpos);
     }
     
     void declaration_action(void)
     { curpos=SpanOf(_pst[_nsp-1],_pst[_nsp]);
       _pst[_nsp-1]=curpos;
       _nst[_nsp-1]=Mk2Blk(&curpos,_nst[_nsp-1],_nst[_nsp]);
       _nsp--;
     }
     
     void statement_action(void)
     { declaration_action(void) }
     
     void block_end(void)
     { _nst[_nsp]=MkBlk(&_pst[_nsp],_nst[_nsp]); }


File: syntax,  Node: Integration,  Prev: Lists,  Up: Foreign

Running a foreign parser under Eli
==================================

   There are two distinct possibilities for the implementation of a
foreign parser:

   * The foreign parser exists as a collection of C/C++ source files
     and/or object files that can be linked with the tree construction
     and computation modules.  (A scanner/parser created by LEX/YACC
     or FLEX/Bison would have this property.)

   * The foreign parser exists as an executable file that expects to
     load a shared library containing the tree construction and
     computation modules.  (A scanner/parser created by a Java-based
     tool like ANTLR would have this property.)

* Menu:

* Linked::  The parser is a collection of routines
* Shared::  The parser is an executable file

