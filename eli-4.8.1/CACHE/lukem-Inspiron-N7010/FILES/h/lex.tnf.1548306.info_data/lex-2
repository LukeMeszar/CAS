This is Info file lex, produced by Makeinfo-1.43 from the input file
/home/lukem/Dropbox/CAS/eli-4.8.1/Eli/pkg/gla/lex.tnf.


File: lex,  Node: Available Descriptions,  Next: Definitions of Descriptions,  Up: Canned Descriptions

Available Descriptions
======================

   Each of the identifiers in the following list is the name of a
canned description specifying the lexical structure of some component
of an existing programming language.  Here they are simply
characterized by the role they play in that language.  A complete
definition of each, consisting of a regular expression, possibly an
auxiliary scanner name, and possibly a token processor name, is given
in the next section.

   When building a new language, it is a good idea to use canned
descriptions for lexical components: Time is not wasted in deciding on
their form, mistakes are not made in their implementation, and users
are familiar with them.

   The list also provides canned descriptions for spaces, tabs and
newlines.  These white space characters are treated as comments by
default.  If, however, you define any pattern that will accept a white
space character in its first position, this pattern overrides the
default treatment and that white space character will be accepted
*only* in contexts that are specified explicitly (*note Spaces Tabs
and Newlines: White Space.).  For example, suppose that the following
pattern were defined and that no other patterns contain spaces:

     Separator:  $\040+#\040+

In that situation, a space will be accepted only if it is part of a
`Separator'.  To treat spaces that are not part of a `Separator' as
comments, include the canned description `SPACES' as a comment
specification:

     Separator:  $\040+#\040+
                 SPACES

   Note that only a white space character that appears at the
beginning of a pattern loses its default interpretation in this way. 
In this example, neither the tab nor the newline appeared at the
beginning of a pattern and therefore tabs and newlines continue to be
treated as comments.

`C_IDENTIFIER, C_INTEGER, C_INT_DENOTATION, C_FLOAT, C_STRING_LIT, C_CHAR_CONSTANT, C_COMMENT'
     Identifiers, integer constants, floating point constants, string
     literals, character literals, and comments from the C programming
     language, respectively.

     `C_INTEGER' does not permit the L or U flags, but does correctly
     accept all other C integer denotations.  By default, it uses
     `c_mkint' to convert the denotation to an internal `int' value. 
     `c_mkint' obeys the C rules for determining the radix of the
     conversion.

     `C_INT_DENOTATION' accepts all valid ANSI C integer denotations. 
     By default, it uses `mkstr' to deliver a unique string table
     index for every occurrence of a denotation.  This behavior is
     often overridden by adding `[mkidn]':

          Integer:  C_INT_DENOTATION [mkidn]

     In this case, two identical denotations will have the same string
     table index.

`C_IDENTIFIER_ISO'
     Character sequences obeying the the definition of a C identifier,
     but accepting all ISO/IEC 8859-1 letters.  Care must be taken in
     using this description because these identifiers are not
     acceptable to most C compilers.  That means they cannot usually
     be used as (parts of) identifiers in generated code.

`PASCAL_IDENTIFIER, PASCAL_INTEGER, PASCAL_REAL, PASCAL_STRING, PASCAL_COMMENT'
     Identifiers, integer constants, real constants, string literals,
     and comments from the Pascal programming language, respectively.

`MODULA2_INTEGER, MODULA2_CHARINT, MODULA2_LITERALDQ, MODULA2_LITERALSQ, MODULA2_COMMENT'
     Integer constants, characters specified using character codes,
     string literals delimited by double and single quotes, and
     comments from the Modula-2 programming language, respectively.

`MODULA3_COMMENT'
     Comments from the Modula-3 programming language.

`ADA_IDENTIFIER, ADA_COMMENT'
     Identifiers and comments from the Ada programming language.

`AWK_COMMENT'
     Comments from the AWK programming language.

`SPACES'
     Sequence of one or more spaces.

`TAB'
     A single horizontal tab.

`NEW_LINE'
     A single newline.


File: lex,  Node: Definitions of Descriptions,  Prev: Available Descriptions,  Up: Canned Descriptions

Definitions of Canned Descriptions
==================================

   Eli textually replaces a reference to a canned description with its
definition.  If a user nominates an auxiliary scanner and/or a token
processor for a canned description, that overrides the corresponding
nomination appearing in the definition of the canned description.

   The following is an alphabetized list of the canned descriptions
available in the Eli library, with their definitions.  Use this list
as a formal definition, and as an example for constructing
specifications.  (`C_FLOAT' and `PASCAL_REAL' have definitions that
are too long to fit on one line of this document.  Each is, however, a
single line in the specification file.)

`ADA_COMMENT'
     `$-- (auxEOL)'

`ADA_IDENTIFIER'
     `$[a-zA-Z](_?[a-zA-Z0-9])* [mkidn]'

`AWK_COMMENT'
     `$# (auxEOL)'

`C_COMMENT'
     `$"/*" (auxCComment)'

`C_CHAR_CONSTANT'
     `$' (auxCChar) [c_mkchar]'

`C_FLOAT'
     `$((([0-9]+\.[0-9]*|\.[0-9]+)((e|E)(\+|-)?[0-9]+)?)|
     ([0-9]+(e|E)(\+|-)?[0-9]+))[fFlL]? [mkstr]'

`C_IDENTIFIER'
     `$[a-zA-Z_][a-zA-Z_0-9]* [mkidn]'

`C_INTEGER'
     `$([0-9]+|0[xX][0-9a-fA-F]*) [c_mkint]'

`C_INT_DENOTATION'
     `$([1-9][0-9]*|0[0-7]*|0[xX][0-9a-fA-F]+)([uU][lL]?|[lL][uU]?)?
     [mkstr]'

`C_STRING_LIT'
     `$\" (auxCString) [mkstr]'

`MODULA_INTEGER'
     `$[0-9][0-9A-Fa-f]*[BCH]? [modula_mkint]'

`MODULA2_COMMENT, MODULA3_COMMENT'
     `$\(\* (auxM3Comment)'

`MODULA2_CHARINT'
     `$[0-9][0-9A-Fa-f]*C [modula_mkint]'

`MODULA2_INTEGER'
     `$[0-9][0-9A-Fa-f]*[BH]? [modula_mkint]'

`MODULA2_LITERALDQ'
     `$\" (auxM2String) [mkstr]'

`MODULA2_LITERALSQ'
     `$\' (auxM2String) [mkstr]'

`PASCAL_COMMENT'
     `$"{"|"(*" (auxPascalComment)'

`PASCAL_IDENTIFIER'
     `$[a-zA-Z][a-zA-Z0-9]*  [mkidn]'

`PASCAL_INTEGER'
     `$[0-9]+  [mkint]'

`PASCAL_REAL'
    
    
     `$(([0-9]+\.[0-9]+)((e|E)(\+|-)?[0-9]+)?)|([0-9]+(e|E)(\+|-)?[0-9]+)
     [mkstr]'

`PASCAL_STRING'
     `$' (auxPascalString) [mkstr]'

`SPACES'
     `$\040+'

`TAB'
     `$\t (auxTab)'

`NEW_LINE'
     `$[\r\n] (auxNewLine)'


File: lex,  Node: White Space,  Next: Literal Symbols,  Prev: Canned Descriptions,  Up: Top

Spaces, Tabs and Newlines
*************************

   An Eli-generated processor examines its input text sequentially,
recognizing character sequences in the order in which they appear.  At
each point it matches the longest possible sequence, classifies that
sequence, and then begins anew with the next character.  If the first
character of a sequence is a space, tab or newline then the default
behavior is to classify the sequence consisting of that character and
all succeeding spaces, tabs and newlines as a comment.  This behavior
is consistent with the definitions of most programming languages, and
is reasonable in a large fraction of text processing tasks.

   Even though tabs and newlines are considered comments by default,
some processing is needed to account for their effect on the source
text position.  Eli-generated processors define a two-dimensional
coordinate system (line number and column index), which they use to
link error reports to the source text (*note Source Text Coordinates
and Error Reporting: (lib)error.).

   White space may be significant in two situations:

  1. Within a character sequence, such as spaces in a string

  2. On its own, such as line boundaries in a type-`gla' file

Appropriate white space may be specified as part of the description of
a complete character sequence (provided that it is not at the
beginning) without disrupting the default behavior.  (Coordinate
processing for tabs and newlines must be provided if they are allowed
within the sequence.) The default behavior is overridden, however, by
any specification of white space on its own or at the beginning of
another character sequence.  Overriding is specific to the white space
character used: a specification of new behavior for a space overrides
the default behavior for a space, but not the default behavior for a
tab or newline.

* Menu:

* Coordinates::	Maintaining the source text coordinates
* Skipping::	Restoring the default behavior for white space
* Illegal::	Making White Space Illegal


File: lex,  Node: Coordinates,  Next: Skipping,  Up: White Space

Maintaining the Source Text Coordinates
=======================================

   The raw data for determining coordinates are two variables,
`LineNum' (an integer variable exported by the error module, *note
Source Text Coordinates and Error Reporting: (lib)error.) and
`StartLine' (a character pointer exported by the lexical analyzer). 
The following invariant must be maintained on these variables:

     LineNum=Cumulative index of the current line in the input text
     (Pointer to current character)-StartLine=index of the current character
         in the current line

This invariant must hold whenever the lexical analyzer begins to
process a character sequence.  It may be destroyed during the
processing of that sequence, but must be re-established before
processing of the next character sequence begins.

* Menu:

* Updating with code::		How the invariant is maintained
* Updating with scanners::	Auxiliary scanners maintaining the invariant


File: lex,  Node: Updating with code,  Next: Updating with scanners,  Up: Coordinates

How the invariant is maintained
...............................

   `LineNum' is initially 1, and must be incremented each time the
lexical analyzer advances beyond a newline character in the input text. 
At the beginning of each line, `StartLine' must be set to point to the
character position preceding the first character of that line.  As the
current character pointer is advanced, the condition on `StartLine' is
maintained automatically unless the character pointer advances over a
tab character.

   A tab character in the input text represents one or more spaces,
depending upon its position relative to the next tab stop, but it
occupies only one character position.  If the tab represents N spaces,
N-1 must be subtracted from `StartLine' to maintain the invariant.

   Because the value of N depends upon the index of the current
character and the settings of the tab stops in the line, Eli provides
an operation `TABSIZE(i)' (defined in file `tabsize.h') to compute it. 
The argument `i' is the index in the current line of the character
position beyond that containing the tab, and the result is the number
of spaces that must be added to reach the next tab stop.

   Suppose that `p' is a pointer to the current input character.  Here
is a code sequence that maintains the condition on `StartLine' when a
tab is encountered:

     #include "tabsize.h"
     ...
       if ((*p++) == '\t') StartLine -= TABSIZE(p - StartLine);
     ...

   `TABSIZE' defines the positions of the tab stops.  The default
implementation provides tab stops every 8 character positions.  A user
changes this default by supplying a new version of the Eli library
routine `TabSize'.  The source code for the library version of this
routine can be obtained by making the following request:

     -> $elipkg/gla/tabsize.c > MyTabSize.c

After modifying the routine appropriately, add the name `MyTabSize.c'
to your type-`specs' file.


File: lex,  Node: Updating with scanners,  Prev: Updating with code,  Up: Coordinates

Auxiliary scanners maintaining the invariant
............................................

   The coordinate invariant is maintained automatically if no patterns
matching tabs or newline characters are defined, and no auxiliary
scanners that advance over tabs or newline characters are provided by
the user.  If such patterns or scanners are needed, then the user must
define them in such a way that they maintain the coordinate invariant.

   Three auxiliary scanners (`coordAdjust', `auxTab' and `auxNewLine')
are available to maintain the coordinate invariant for a regular
expression that matches tabs or newline characters (*note Available
scanners::.).  While these auxiliary scanners could be invoked by
user-defined auxiliary scanners that advance over tabs or newline
characters, it is often simpler to include the appropriate code to
maintain the coordinate invariant.

   For an example of the use of code in an auxiliary scanner to
maintain the coordinate invariant, see the library version of `auxNUL'.


File: lex,  Node: Skipping,  Next: Illegal,  Prev: Coordinates,  Up: White Space

Restoring the Default Behavior for White Space
==============================================

   When a pattern beginning with a space, tab or newline character
overrides the default behavior for that character, the character will
only be accepted as part of an explicit pattern.  The default behavior
can be restored by using one of the canned descriptions `SPACES',
`TAB' or `NEW_LINE' respectively (*note Available Descriptions::.):

     Define:  $\040+define
              SPACES

Here the pattern for `Define' overrides the default behavior for space
characters.  If this were the only specification, spaces in the input
text would only be accepted if they occurred immediately before the
character sequence `define'.  By adding the canned description
`SPACES', and classifying the sequences it matches as comments, the
default behavior is restored.

   Note that this specification is ambiguous: A sequence of spaces
followed by `define' could either match the `Define' pattern or the
spaces alone could be classified as the comment specified by `SPACES'. 
The principle of the longest match guarantees that in this case the
sequence will be classified as `Define' (*note Ambiguity::.).


File: lex,  Node: Illegal,  Prev: Skipping,  Up: White Space

Making White Space Illegal
==========================

   When white space is illegal at the beginning of a pattern, the
default treatment of white space must be overridden with an explicit
comment pattern.  Because the sequence is specified to be a comment,
nothing will be returned to the parser.  A token processor like
`lexerr' can be used to report the error:

     	SPACES	[lexerr]

   The canned descriptions `SPACES', `TAB' and `NEW_LINE' should be
used as patterns in such specifications because they handle all of the
coordinate updating (*note Maintaining the source text coordinates:
Coordinates.).


File: lex,  Node: Literal Symbols,  Next: Case Insensitivity,  Prev: White Space,  Up: Top

Literal Symbols
***************

   If the generated processor includes a parser (*note Parsing:
(syntax)Top.), then Eli will extract the descriptions of any literal
terminal symbols from the context-free grammar defining that parser
and add them to the specifications provided by type-`gla' files.  For
example, consider the following context-free grammar:

     Program: Expression .
     Expression: Evaluation / Binding .
     Evaluation:
       Constant / BoundVariable /
       '(' Expression '+' Expression ')' /
       '(' Expression '*' Expression ')' .
     Binding: 'let' BoundVariable '=' Evaluation 'in' Expression .

This grammar has nine terminal symbols.  Two (`Constant' and
`BoundVariable') are given by identifiers, and the other seven (`(',
`)', `+', `*', `let', `=' and `in') are given by literals.

   Only the character sequences to be classified as `Constant' or
`BoundVariable', and those to be classified as comments, need be
defined by type-`gla' files.  Descriptions of the symbols given as
literals will be automatically extracted from the grammar by Eli. 
Thus the lexical analyzer for this language might be described by a
single type-`gla' file containing the following:

     Constant:      PASCAL_INTEGER
     BoundVariable: PASCAL_IDENTIFIER
                    PASCAL_COMMENT

* Menu:

* Overriding::	Overriding the Default Treatment of Literal Symbols
* Surrogates::	Using Literal Symbols to Represent Other Things


File: lex,  Node: Overriding,  Next: Surrogates,  Up: Literal Symbols

Overriding the Default Treatment of Literal Symbols
===================================================

   By default, a literal terminal symbol specified in a context-free
grammar supplied to Eli will be recognized as though it had been
specified by the appropriate regular expression.  Thus the literal
symbols `'+'' and `'let'' will be recognized as though the following
specifications had been given by the user:

     Plus:  $\+
     Let:   $let

(Here `Plus' and `Let' are arbitrary identifiers describing the
initial classifications of the literal symbols.  No such identifiers
are actually supplied by Eli, but the literal symbols are *not*
initially classified as comments.)

   In some situations it is useful to carry out more complex
operations at the time the literal symbol is recognized.  In this
case, the user must do two things:

  1. Mark the literal symbol as being a special case.

  2. Provide a specification for the literal symbol.

* Menu:

* Override example::	A situation in which a complex operation is useful
* The delit file::	Marking the literal symbol as a special case
* Specifying behavior::	Providing a specification for the literal symbol


File: lex,  Node: Override example,  Next: The delit file,  Up: Overriding

A situation in which a complex operation is useful
..................................................

   As a concrete example, suppose that `%%' were used as a major
separator in the input text and could appear either once or twice. 
Assume that the first occurrence is required, and the second is
optional.  All text following the second occurrence is to be ignored.

   One approach to this problem would be to count the number of
occurrences of the literal symbol `%%', advancing to the end of the
input text after the second.  This could be done by an auxiliary
scanner (*note Auxiliary Scanners::.) that either returns a pointer to
the character following the `%%' or a pointer to the ASCII NUL
terminating the input text, and a token processor (*note Token
Processors::.) that reclassifies the second occurrence of `%%' as a
comment.  The grammar would specify only the required first occurrence
of `%%'.


File: lex,  Node: The delit file,  Next: Specifying behavior,  Prev: Override example,  Up: Overriding

Marking the literal symbol as a special case
............................................

   In order to mark the literal symbol `%%' as a special case that
should not receive the default treatment, the user must supply a
type-`delit' file specifying that symbol as a regular expression.  The
entry in the type-`delit' file also needs to define an identifier to
represent the classification:

     $%%  PercentPercent

   Each line of a type-`delit' file consists of a regular expression
and an identifier, separated by white space.  The regular expression
must describe a literal symbol appearing in a context-free grammar
supplied to Eli.  That literal symbol will not be incorporated
automatically into the generated lexical analyzer; it must be
specified explicitly by the user.  The identifier will be given the
appropriate value by an Eli-generated `#define' directive in file
`litcode.h'.


File: lex,  Node: Specifying behavior,  Prev: The delit file,  Up: Overriding

Providing a specification for the literal symbol
................................................

   In our example, `%%' could be specified by the following line of a
type-`gla' file:

       $%%  (SkipOrNot) [CommentOrNot]

Initially, the separator will be classed as a comment because there is
no identifier preceding the regular expression.  `SkipOrNot' will use
a state variable to decide whether or not to skip text (*note Building
scanners::.), while `CommentOrNot' will use the same state variable to
decide whether or not to change the classification to `PercentPercent'
(*note Building Processors::.):

     #include <fcntl.h>
     #include "source.h"
     #include "litcode.h"
     
     static int Second = 0;
     
     char *
     SkipOrNot(char *start, int length)
     { if (!Second) return start + length;
       (void)close(finlBuf());
       initBuf("/dev/null", open("/dev/null", O_RDONLY));
       return TEXTSTART;
     }
     
     void
     CommentOrNot(char *start, int length, int *syncode, int *intrinsic)
     { if (!Second) { Second++; *syncode = PercentPercent; }
     }

The remainder of the text is skipped by closing the current input file
and opening an empty file to read (*note Text Input: (lib)source.). 
Since `%%' is initially classified as a comment, its classification
must be changed only on the first occurrence.

   File `fcntl.h' defines `open' and `O_RDONLY', `source.h' defines
`initBuf', `finlBuf' and `TEXTSTART', and `litcode.h' defines
`PercentPercent'.


File: lex,  Node: Surrogates,  Prev: Overriding,  Up: Literal Symbols

Using Literal Symbols to Represent Other Things
===============================================

   In some cases the phrase structure of a language depends upon
layout cues rather than visible character sequences.  For example,
indentation is used in Occam2 to indicate block structure: If the
first non-blank character of a line is indented further than the first
non-blank character of the line preceding it, then the new line begins
a new block.  If the first non-blank character of a line is not
indented as far as the first non-blank character of the line preceding
it, then the old line ends one or more blocks depending on the
difference in indentation.  If the first non-blank characters of two
successive lines are indented by the same amount, then the lines
simply contain adjacent statements of the same block.

   Layout cues can be represented by literal symbols in the
context-free grammar that describes the phrase structure.  The
processing needed to recognize the layout cues can then be described in
any convenient manner, and the sequence of white space characters
implementing those cues can be classified as the appropriate literal
symbol.

   Suppose that the beginning of a block is represented in the Occam2
grammar by the literal symbol `'{'', the statement separator by `';'',
and the end of a block by `'}''.  In the input text, blocks and
statement separators are defined by layout cues as described above.  A
type-`delit' file marks the literal symbols as requiring special
recognition and associates an identifier with each:

     $\{  Initiate
     $;  Separate
     $\}  Terminate

   Indentation can be specified as white space following a new line:

       $\n[\t\040]*  [OccamIndent]

The token processor `OccamIndent' would carry out all of the processing
necessary to determine the meaning of the indentation.  This
processing is complex, involving interactions with several other
components of the generated lexical analyzer (*note An Example of
Interface Usage: Occam.).  It constitutes an operational definition of
the meaning of indentation in Occam2.


File: lex,  Node: Case Insensitivity,  Next: Generated Module,  Prev: Literal Symbols,  Up: Top

Case Insensitivity
******************

   The default behavior of an Eli-generated lexical analyzer is to
treat each ASCII character as an entity distinct from all other ASCII
characters.  This behavior is inappropriate for applications that do
not distinguish upper-case letters from lower-case letters in certain
contexts.  For example, a Pascal compiler ignores the case of letters
in identifiers and keywords, but distinguishes them in strings.  Thus
the Pascal identifiers `MyId', `MYID' and `myid' are identical but the
strings `'MyString'', `'MYSTRING'' and `'mystring'' are different.

   Case insensitivity is reflected in the identity of character
sequences.  In other words, the character sequences `MyId', `MYID' and
`myid' are considered to be identical character sequences if and only
if the generated processor is insensitive to the case of letters.  Two
character sequences are identical as far as the remainder of the
processor is concerned if they have the same classification and their
values are equal (*note Specifications::.).  Since the classification
and value are determined by the token processor, it is the token
processor that must implement case insensitivity.

   Two conditions must be met if a processor is to be insensitive to
case:

  1. A token processor that maintains a table of character sequences
     in which all letters are of one case must be available.

  2. The specification of each case-insensitive character sequence
     must invoke such a token processor.

* Menu:

* Folding::	A Case-Insensitive Token Processor
* Keywords::	Making Literal Symbols Case Insensitive


File: lex,  Node: Folding,  Next: Keywords,  Up: Case Insensitivity

A Case-Insensitive Token Processor
==================================

   The token processor `mkidn' maintains a table of character sequences
and provides the same classification and value for identical character
sequences.  Normally, `mkidn' treats upper-case letters and lower-case
letters as different characters.  This behavior is controlled by an
exported variable, `dofold' (*note Unique Identifier Management:
(lib)identifier.): When `dofold=0' character sequences are entered
into the table as they are specified to `mkidn'; otherwise all letters
in the sequence are converted to upper case before the sequence is
entered into the table.

   Although the value of `dofold' could be altered on the basis of
context by user-defined code, it is normally constant throughout the
processor's execution.  To generate a processor in which `dofold=1',
specify the parameter `+fold' in the request (*note fold -- Make the
Processor Case-Insensitive: (pp)fold.).  If this parameter is not
specified in the request, Eli will produce a processor with `dofold=0'.

   The value set by `mkidn' is the (unique) index of the transformed
character sequence in the table.  Thus if that value is used to
retrieve the sequence at a later time, the result will be the original
sequence with all lower-case letters replaced by their upper-case
equivalents.


File: lex,  Node: Keywords,  Prev: Folding,  Up: Case Insensitivity

Making Literal Symbols Case Insensitive
=======================================

   Since literal symbols are recognized exactly as they stand in the
grammar, they are case sensitive by definition.  For example, if a
grammar for Pascal contains the literal symbol `'begin'' then the
generated processor will recognize only the character sequence `begin'
as an instance of that literal symbol.  This behavior could be changed
by redefining the literal symbol as a nonliteral symbol (say) `BEGIN',
and providing the following specification in a type-`gla' file:

     BEGIN:  $[Bb][Ee][Gg][Ii][Nn]  [mkidn]

If the number of literal symbols to be treated as case-insensitive is
large, this is a very tedious and error-prone approach.  It also
distorts the grammar by converting literal terminal symbols to
non-literal terminal symbols.

   To solve this problem, Eli allows the user to specify a set of
literal symbols that should be placed into the table used by `mkidn',
with their classification codes, at the time the generated lexical
analyzer is loaded.  If the `+fold' parameter is also specified, all
lower-case letters in these symbols will be replaced by their
upper-case equivalents before the symbol is placed into the table. 
The desired behavior is then obtained by invoking `mkidn' after
recognizing the appropriate character sequence in the input text.

   The set of literal symbols to be placed into the table is specified
by giving a sequence of regular expressions in a type-`gla' file, and
then deriving the `:kwd' product from that file (*note kwd --
Recognize Specified Literals as Identifiers: (pp)kwd.).  The regular
expressions describe the form of the literal symbols in the grammar,
*not* the input character sequences to be recognized.

   Suppose, for example, that a Pascal grammar specified all keywords
as literal symbols made up of lower-case letters:

     Statement:
       ...
       'while' Expression 'do' Statement /
       ...

A type-`gla' file describing the form these symbols take in the
grammar would consist of the single line `$[a-z]+'.  If the name of
that file was `PascalKey.gla' then the user could tell Eli to
initialize `mkidn''s table with all of the keywords by including the
following line in a type-`specs' file:

     PascalKey.gla :kwd

   In Pascal, keywords have the form of identifiers in the input text. 
Therefore the canned description `PASCAL_IDENTIFIER' suffices to
recognize both identifiers and keywords.  `PASCAL_IDENTIFIER' invokes
`mkidn' to obtain the classification and value of the sequence
recognized by the regular expression `$[a-zA-Z][a-zA-Z0-9]*'.  Since
`mkidn''s table has been initialized with the character sequences for
the literal keyword symbols, and their classifications, they will be
appropriately recognized.

   The `:kwd' product and the `+fold' parameter are independent of one
another.  Thus, in order to make the generated lexical analyzer accept
Pascal keywords with arbitrary case the user must both provide the
`:kwd' specification and derive with the `+fold' parameter.


File: lex,  Node: Generated Module,  Next: Index,  Prev: Case Insensitivity,  Up: Top

The Generated Lexical Analyzer Module
*************************************

   This chapter discusses the generated lexical analyzer module, its
interface, and its relationship to other modules in the generated
processor.  An understanding of the material here is not necessary for
normal use of the lexical analyzer.

   There are some special circumstances in which it is necessary to
change the interactions between the lexical analyzer and its
environment.  For example, there is a mismatch between the lexical
analyzer and the source code input module of a FORTRAN 90 compiler:
The unit of input text dealt with by the source code module is the
line, the unit dealt with by the lexical analyzer is the statement,
and there is no relationship between lines and statements.  One line
may contain many statements, or one statement may be spread over many
lines.  This mismatch problem is solved by requiring the two modules
to interact via a buffer, and managing that buffer so that it contains
both an integral number of lines and an integral number of statements. 
Because the lexical analyzer normally works directly in the source
module's buffer, that solution requires a change in the relationship
between the lexical analyzer and its environment.

   The interaction between the lexical analyzer and its environment is
governed by the following interface:

     #include "gla.h"
     /* Entities exported by the lexical analyzer module
      * NORETURN	(constant)	Classification of a comment
      * ResetScan	(variable)	Flag causing scan pointer reset
      * TokenStart	(variable)	Address of first classified character
      * TokenEnd	(variable)	Address of first unclassified character
      * StartLine	(variable)	Column index = (TokenEnd - StartLine)
      * glalex	(operation)	Classify the next character sequence
      ***/

* Menu:

* Text::	How the lexical analyzer interacts with the text
* Reset::	How the scan pointer is reset
* Classify::	How the next character sequence is classified
* Occam::	An Example of Interface Usage


File: lex,  Node: Text,  Next: Reset,  Up: Generated Module

Interaction Between the Lexical Analyzer and the Text
=====================================================

   There is no internal storage for text in the lexical analyzer
module.  Instead, `TokenEnd' is set to point to arbitrary text storage. 
(Normally the pointer is to the source buffer, *note Text Input:
(lib)source..) The text pointed to must be an arbitrary sequence of
characters, the last of which is an ASCII NUL.

   At the beginning of a scan, `TokenEnd' points to the beginning of
the string on which a sequence is to be classified.  The lexical
analyzer tests that string against its set of regular expressions,
finding the longest sequence that begins with the first character and
matches one of the regular expressions.

   If the regular expression matched is associated with an auxiliary
scanner then that auxiliary scanner is invoked with the matched
sequence (*note Building scanners::.).  The auxiliary scanner returns
a pointer to the first character that should not be considered part of
the character sequence being matched, and that pointer becomes the
value of `TokenEnd'.  `TokenStart' is set to point to the first
character of the string.

   When no initial character sequence matches any of the regular
expressions an error report is issued, `TokenEnd' is advanced by one
position (thus discarding the first character of the string), and the
process is restarted.  If the string is initially empty, no attempt is
made to match any regular expressions.  Instead, the auxiliary scanner
`auxNUL' is invoked immediately.  If this auxiliary scanner returns a
pointer to an empty string then the auxiliary scanner `auxEOF' is
invoked immediately.  Finally, if `auxEOF' returns a pointer to an
empty string then the Token processor `EndOfText' is invoked
immediately.  (If either `auxNUL' or `auxEOF' returns a pointer to a
non-empty string, scanning begins on this string as though `TokenEnd'
had pointed to it initially.)

   `TokenStart' addresses a sequence of length `TokenEnd-TokenStart'
when a token processor is invoked (*note Building Processors: Token
Processors.).  Because `TokenStart' and `TokenEnd' are exported
variables, the token processor may change them if that is appropriate. 
All memory locations below the location pointed to by `TokenStart' are
undefined in the fullest sense of the word: Their contents are
unknown, and they may not even exist.  Memory locations beginning with
the one pointed to by `TokenStart', up to but not including the one
pointed to by `TokenEnd', are known to contain a sequence of non-NUL
characters.  `TokenEnd' points to a sequence of characters, the last
of which is an ASCII NUL.  If the token processor modifies the
contents of `TokenStart' or `TokenEnd', it must ensure that these
conditions hold after the modification.


File: lex,  Node: Reset,  Next: Classify,  Prev: Text,  Up: Generated Module

Resetting the Scan Pointer
==========================

   If the exported variable `ResetScan' is non-zero when the operation
`glalex' is invoked, the lexical analyzer's first action is to execute
the macro `SCANPTR'.  `SCANPTR' guarantees that `TokenEnd' addresses
the string to be scanned.  If `ResetScan' is zero when `glalex' is
invoked, `TokenEnd' is assumed to address that string already. 
`ResetScan' is statically initialized to `1', meaning that `SCANPTR'
will be executed on the first invocation of `glalex'.

   In the distributed system, `SCANPTR' sets `TokenEnd' to point to
the first character of the source module's text buffer.  Since this is
also the first character of a line, `StartLine' must also be set
(*note Maintaining the Source Text Coordinates: Coordinates.):

     #define SCANPTR { TokenEnd = TEXTSTART; StartLine = TokenEnd - 1; }

*Note Text Input: (lib)source.  This implementation can be changed by
supplying a file `scanops.h', containing a new definition of `SCANPTR',
as one of your specification files.

   `ResetScan' is set to zero after `SCANPTR' has been executed. 
Normally, it will never again have the value `1'.  Thus `SCANPTR' will
not be executed on any subsequent invocation of `glalex'.  Periodic
refilling of the source module's text buffer and associated re-setting
of `TokenEnd' is handled by `auxNUL' when the lexical analyzer detects
that the string is exhausted.  More complex behavior, using
`ResetScan' to force resets at arbitrary points, is always possible
via token processors or other clients.

   `TokenEnd' is statically initialized to `0'.  Once scanning has
begun, `TokenEnd' should always point to a location in the source
buffer (*note Text Input: (lib)source.).  Thus `SCANPTR' can normally
distinguish between initialization and arbitrary re-setting by testing
`TokenEnd'.  (If user code sets `TokenEnd' to `0', of course, this
test may not be valid.)


File: lex,  Node: Classify,  Next: Occam,  Prev: Reset,  Up: Generated Module

The Classification Operation
============================

   The classification operation `glalex' is invoked with a pointer to
an integer variable that may be set to the value representing the
classified sequence.  An integer result specifying the classification
is returned by `glalex', and the coordinates of the first character of
the sequence are stored in the error module's exported variable
`curpos' (*note Source Text Coordinates and Error Reporting:
(lib)error.).

   There are three points at which these interactions can be altered:

  1. Setting coordinate values

  2. Deciding on a continuation after a classification

  3. Returning a classification

All of these alterations are made by supplying macro definitions in a
specification file called `scanops.h'.  The remainder of this section
defines the macro interfaces and gives the default implementations.

* Menu:

* Position::	Setting coordinate values
* Continue::	Deciding on a continuation after a classification
* Return::	Returning a classification


File: lex,  Node: Position,  Next: Continue,  Up: Classify

Setting coordinate values
-------------------------

   The coordinates of the first character of a sequence are set by the
macro `SETCOORD'.  Its default implementation uses the standard
coordinate invariant (*note Maintaining the Source Text Coordinates:
Coordinates.):

     /* Set the coordinates of the current token
      *   On entry-
      *     LineNum=index of the current line in the entire source text
      *     p=index of the current column in the entire source line
      *   On exit-
      *     curpos has been updated to contain the current position as its
      *     left coordinate
      */
     #define SETCOORD(p) { LineOf(curpos) = LineNum; ColOf(curpos) = (p); }

   When execution monitoring (*note Monitoring: (mon)Top.) is in
effect, more care must be taken.  In addition to the above, `SETCOORD'
must also set the cumulative column position, which is the column
position within the overall input stream (as opposed to just the
current input file).  Ordinarily the two column positions will be the
same, so the default implementation of `SETCOORD' for monitoring is:

     #define SETCOORD(p) { LineOf(curpos) = LineNum; \
     		      ColOf(curpos) = CumColOf(curpos) = (p); }

   When monitoring, it is also necessary to set the coordinates of the
first character beyond the sequence.  This is handled by the macro
`SETENDCOORD':

     /* Set the coordinates of the end of the current token
      *   On entry-
      *     LineNum=index of the current line in the entire source text
      *     p=index of the current column in the entire source line
      *   On exit-
      *     curpos has been updated to contain the current position as its
      *     right coordinate
      */
     #ifndef SETENDCOORD
     #define SETENDCOORD(p) { RLineOf(curpos) = LineNum; \
     			 RColOf(curpos) = RCumColOf(curpos) = (p); }
     #endif


File: lex,  Node: Continue,  Next: Return,  Prev: Position,  Up: Classify

Deciding on a continuation after a classification
-------------------------------------------------

   Classification is complete after the regular expression has been
matched, any specified auxiliary scanner invoked, and any specified
token processor invoked.  At this point, one of three distinct actions
is possible:

`RETURN v'
     Terminate the invocation of `glalex', returning the value `v' as
     the classification.

`goto rescan'
     Start a new scan at the character addressed by `TokenEnd',
     without changing the coordinate value.

`continue'
     Start a new scan at the character addressed by `TokenEnd',
     resetting the coordinates to the coordinates of that character.

   `WRAPUP' is the macro responsible for deciding among these
possibilities.  When it is executed, `TokenEnd' addresses the first
character beyond the classified sequence and `extcode' holds the
classification code.  Here is the default implementation:

     #define WRAPUP { if (extcode != NORETURN) RETURN extcode; }

If `WRAPUP' does not transfer control, the result is the `continue'
action.  Thus the default implementation of `WRAPUP' terminates the
invocation of `glalex' if the current character sequence is not
classified as a comment (`extcode != NORETURN'), and starts a new scan
at the next character if the current character sequence is classified
as a comment.

   If execution monitoring is in effect, the classification event must
be reported in addition to selecting a continuation:

     #define WRAPUPMONITOR { \
       if (extcode != NORETURN) { \
         char save = *TokenEnd; \
         *TokenEnd = '\0'; \
         generate_token("token", LineOf(curpos), ColOf(curpos), \
     		    CumColOf(curpos), RLineOf(curpos), RColOf(curpos), \
     		    RCumColOf(curpos), TokenStart, TokenEnd - TokenStart, \
     		    *v, extcode); \
         *TokenEnd = save; \
       } \
     }

`WRAPUPMONITOR' is invoked instead of `WRAPUP' if execution monitoring
is in effect.


File: lex,  Node: Return,  Prev: Continue,  Up: Classify

Returning a classification
--------------------------

   Once the decision has been made to terminate the `glalex' operation
and report the classification, it is possible to carry out arbitrary
operations in addition to returning the classification code.  For
example, execution monitoring requires that this event be reported. 
Here is the default implementation:

     #ifdef MONITOR
     #define RETURN(v) { generate_leave("lexical"); return v; }
     #else
     #define RETURN(v) { return v; }
     #endif


File: lex,  Node: Occam,  Prev: Classify,  Up: Generated Module

An Example of Interface Usage
=============================

   Recognition of Occam2 block structure from indentation is an
example of how a token processor might use the lexical analyzer
interface (*note Using Literal Symbols to Represent Other Things:
Surrogates.).  The token processor `OccamIndent' is invoked after a
newline character (possibly followed by spaces and/or tabs) has been
recognized:

     #include "err.h"
     #include "gla.h"
     #include "source.h"
     #include "litcode.h"
     
     extern char *auxNUL();
     extern char *coordAdjust();
     
     #define MAXNEST 50
     static int IndentStack[MAXNEST] = {1};
     static int *Current = IndentStack;
     
     void
     OccamIndent(char *start, int length, int *syncode, int *intrinsic)
     { if (start[length] == '\0') {
         start = auxNUL(start, length);
         if (start[length] != '\0') { TokenEnd = start; return; };
         TokenEnd = start + length;
       }
     
       if (*TokenEnd == '\0' && Current == IndentStack) return;
     
       { char *OldStart = StartLine;
         int OldLine = LineNum, Position;
     
         (void)coordAdjust(start, length); Position = TokenEnd-StartLine;
         if (*Current == Position) *syncode = Separate;
         else if (*Current < Position) {
           *syncode = Initiate;
           if (Current == IndentStack + MAXNEST)
             message(DEADLY, "Nesting depth exceeded", 0, &curpos);
           *++Current = Position;
         } else {
           *syncode = Terminate; Current--;
           LineNum = OldLine; StartLine = OldStart; TokenEnd = start;
         }
       }
     }

   Since the source buffer is guaranteed only to hold an integral
number of lines (*note Text Input: (lib)source.), `OccamIndent' must
first refill the buffer if necessary.  The library routine `auxNUL'
carries out this task, returning a pointer to the character sequence
passed to it (*note Available scanners::.).  Remember that the
character sequence may be moved in the process of refilling the
buffer, and therefore it is vital to reset both `start' and `TokenEnd'
after the operation.

   If `auxNUL' is invoked and adds characters to the buffer, then those
characters might be white space that should have been part of the
original pattern.  In this case `OccamIndent' can return, having set
`TokenEnd' to point to the first character of the original sequence. 
Since the sequence was initially classified as a comment (because the
specification did not begin with an identifier followed by a colon,
*note Using Literal Symbols to Represent Other Things: Surrogates.),
the overall effect will be to re-scan the newline and the text now
following it.

   If `auxNUL' is invoked but does not add characters to the buffer,
then the newline originally matched is the last character of the file. 
`TokenEnd' should be set to point to the character following the
newline.

   When the end of the file has been reached, and no blocks remain
unterminated, then the newline character has no meaning.  By returning
under these conditions, `OccamIndent' classifies the newline as a
comment.  Otherwise, the character sequence matched by the pattern
must be interpreted on the basis of the indentation it represents.

   Because a single character sequence may terminate any number of
blocks, it may be necessary to interpret it as a sequence of
terminators.  The easiest way to do this is to keep re-scanning the
same sequence, returning one terminator each time, until all of the
relevant blocks have been terminated.  In order to make that possible,
`OccamIndent' must save the current values of the pointer from which
column indexes are determined (`StartLine') and the cumulative line
number (`LineNum').

   The pattern with which `OccamIndent' is associated will match a
character sequence beginning with a newline and containing an arbitrary
sequence of spaces and tabs.  To determine the column index of the
first character following this sequence, apply `coordAdjust' to it
(*note Available scanners::.).  That auxiliary scanner leaves the
character sequence unchanged, but re-establishes the invariant on
`LineNum' and `StartLine' (*note Maintaining the Source Text
Coordinates: Coordinates.).  After the invariant is re-established,
the column index can be computed.

   `Current' points to the element of `IndentStack' containing the
column index of the first character of a line belonging to the current
block.  (If no block has been opened, the value is 1.) When the column
index of the character following the initial white space is equal to
this value, that white space should be classified as a separator. 
Otherwise, if the column index shows an indentation then the white
space should be classified as an initiator and the new column position
should be pushed onto the stack.  Stack overflow is a deadly error,
making further processing impossible (*note Source Text Coordinates
and Error Reporting: (lib)error.).  Finally, if the column index shows
an exdentation then the white space should be classified as a
terminator and the column position for the terminated block deleted
from the stack.

   When a newline terminates a block, it must be re-scanned and
interpreted in the context of the text surrounding the terminated
block.  Therefore in this case `StartLine' and `LineNum' are restored
to the values they had before `coordAdjust' was invoked, and
`TokenStart' is set to point to the newline character at the start of
the sequence.  Thus the next invocation of the lexical analyzer will
again recognize the sequence and invoke `OccamIndent' to interpret it.

